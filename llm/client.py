"""LLM å®¢æˆ·ç«¯æ¨¡å—

è´Ÿè´£è°ƒç”¨ NVIDIA API ç”Ÿæˆè®ºæ–‡æ‘˜è¦
"""

import time
from typing import Optional
import httpx
from openai import OpenAI

from config.settings import Settings
from utils.helpers import extract_clean_summary, validate_summary


class LLMClient:
    """LLM å®¢æˆ·ç«¯ç±»"""

    def __init__(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        Settings.validate()

        self.http_client = httpx.Client(timeout=Settings.HTTP_TIMEOUT)
        self.client = OpenAI(
            base_url=Settings.NVIDIA_API_BASE_URL,
            api_key=Settings.NVIDIA_API_KEY,
            http_client=self.http_client
        )

    def generate_summary(
        self,
        paper: dict,
        max_retries: Optional[int] = None
    ) -> str:
        """
        ç”Ÿæˆè®ºæ–‡æ‘˜è¦

        Args:
            paper: è®ºæ–‡å­—å…¸ï¼ŒåŒ…å« title, authors, abstract ç­‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼

        Returns:
            ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        max_retries = max_retries or Settings.LLM_MAX_RETRIES

        title = paper['title']
        author = paper['authors'][0] if paper['authors'] else 'Unknown'

        prompt = self._build_prompt(paper)

        print(f"æ­£åœ¨ç ”è¯»è®ºæ–‡: {title}")

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=Settings.NVIDIA_MODEL,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIè®ºæ–‡ç¿»è¯‘åŠ©æ‰‹ã€‚åªè¾“å‡ºæ ¼å¼åŒ–çš„ä¸­æ–‡æŠ¥å‘Šï¼Œä¸è¦è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–åˆ†æè¯´æ˜ã€‚"
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=Settings.LLM_TEMPERATURE,
                    max_tokens=Settings.LLM_MAX_TOKENS,
                )

                content = response.choices[0].message.content

                if content:
                    # æ¸…ç†è¾“å‡ºï¼Œæå–å¹²å‡€çš„æŠ¥å‘Š
                    cleaned = extract_clean_summary(content)

                    # éªŒè¯æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€ç« èŠ‚
                    missing = validate_summary(cleaned)
                    if missing:
                        print(f"âš  æŠ¥å‘Šä¸å®Œæ•´ï¼Œç¼ºå°‘ç« èŠ‚: {', '.join(missing)}")
                        if attempt < max_retries - 1:
                            print(f"å°è¯• {attempt + 1} ä¸å®Œæ•´ï¼Œå‡†å¤‡é‡è¯•...")
                            time.sleep(Settings.LLM_RETRY_DELAY)
                            continue

                    return cleaned
                else:
                    print("âš  API è¿”å›ç©ºå†…å®¹")
                    return "âš  æ¨¡å‹è¿”å›ç©ºå†…å®¹ï¼Œè¯·é‡è¯•"

            except Exception as e:
                print(f"âš  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(Settings.LLM_RETRY_DELAY)
                else:
                    return f"âš  API è¯·æ±‚å¤±è´¥: {str(e)}"

        return "âš  ç”Ÿæˆæ‘˜è¦å¤±è´¥"

    def _build_prompt(self, paper: dict) -> str:
        """æ„å»ºç”Ÿæˆæ‘˜è¦çš„ prompt"""
        return f"""ä»»åŠ¡ï¼šç¿»è¯‘ä»¥ä¸‹è‹±æ–‡è®ºæ–‡æ‘˜è¦ä¸ºä¸­æ–‡æŠ€æœ¯ç®€æŠ¥ã€‚

å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š

## ğŸ“„ è®ºæ–‡æ ‡é¢˜ï¼š[ä¸­æ–‡ç¿»è¯‘çš„æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{paper['title']}
**ç¬¬ä¸€ä½œè€…**ï¼š{paper['authors'][0] if paper['authors'] else 'Unknown'} | **æœºæ„**ï¼šæœªçŸ¥

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[ç”¨ä¸­æ–‡ç¿»è¯‘è®ºæ–‡æ‘˜è¦ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§]

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* [æ ¹æ®æ‘˜è¦åˆ—å‡º3ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹]
* [åˆ›æ–°ç‚¹2]
* [åˆ›æ–°ç‚¹3]

### ğŸ§ ç®€è¯„ä¸å¯ç¤º
[ä¸€å¥è¯æ€»ç»“è®ºæ–‡ä»·å€¼]

è®ºæ–‡æ‘˜è¦ï¼š{paper['abstract']}

è¾“å‡ºè¦æ±‚ï¼š
1. åªè¾“å‡ºæ ¼å¼åŒ–çš„æŠ¥å‘Šå†…å®¹
2. ä¸è¦è¾“å‡ºä»»ä½•åˆ†æã€æ€è€ƒè¿‡ç¨‹ã€æ­¥éª¤è¯´æ˜
3. ä¸è¦è¾“å‡º"å¥½çš„ï¼Œæˆ‘æ¥ç¿»è¯‘"ä¹‹ç±»çš„å¼€å¤´
4. ç›´æ¥ä»"## ğŸ“„"å¼€å§‹è¾“å‡º
5. ç¡®ä¿æ ¸å¿ƒæ‘˜è¦éƒ¨åˆ†æœ‰å®Œæ•´çš„å†…å®¹ï¼Œä¸è¦ç•™ç©º"""

    def close(self):
        """å…³é—­å®¢æˆ·ç«¯è¿æ¥"""
        if self.http_client:
            self.http_client.close()


def generate_paper_summary(paper: dict) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šç”Ÿæˆè®ºæ–‡æ‘˜è¦

    Args:
        paper: è®ºæ–‡å­—å…¸

    Returns:
        ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
    """
    client = LLMClient()
    try:
        return client.generate_summary(paper)
    finally:
        client.close()
