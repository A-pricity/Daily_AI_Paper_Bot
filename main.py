import arxiv
from openai import OpenAI
import datetime
import os
import time
import httpx
from dotenv import load_dotenv
import json
import feedparser
from typing import List, Dict, Optional

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# é…ç½®éƒ¨åˆ†ï¼šä»ç¯å¢ƒå˜é‡è¯»å– NVIDIA API Key
NVIDIA_API_KEY = os.getenv('NVIDIA_API_KEY')
if not NVIDIA_API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ NVIDIA_API_KEY æˆ–åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")

# è·å–ä¼ä¸šå¾®ä¿¡ Webhook URLï¼ˆå¯é€‰ï¼‰
WECHAT_WEBHOOK = os.getenv('WECHAT_WEBHOOK')

# æ•°æ®æºé…ç½®
SOURCES_CONFIG = {
    'arxiv': {
        'enabled': True,
        'base_url': 'http://export.arxiv.org/api/query?',
        'search_topics': [
            "Large Language Models",
            "LLM Agents",
            "Chain of Thought",
            "Batch of Thought",
            "LLM Reasoning"
        ]
    },
    'semantic_scholar': {
        'enabled': False,  # é»˜è®¤å…³é—­ï¼Œéœ€è¦ API Key
        'api_key': os.getenv('SEMANTIC_SCHOLAR_API_KEY'),
        'search_topics': [
            "large language models",
            "LLM agents",
            "reasoning optimization"
        ]
    },
    'springer': {
        'enabled': True,  # Springer æœ‰å…¬å¼€çš„ RSS feeds
        'urls': [
            'https://link.springer.com/rss/journal/volumesandissues/12559',  # Machine Learning
            'https://link.springer.com/rss/journal/volumesandissues/11032',  # Neural Computation
        ]
    }
}

# åˆå§‹åŒ– NVIDIA å®¢æˆ·ç«¯
http_client = httpx.Client(timeout=120.0)
client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=NVIDIA_API_KEY,
    http_client=http_client
)

def get_papers_from_springer(max_results=5) -> List[Dict]:
    """
    ä» Springer RSS feeds è·å–æœ€æ–°è®ºæ–‡
    """
    papers_data = []
    urls = SOURCES_CONFIG['springer'].get('urls', [])

    for url in urls:
        if not SOURCES_CONFIG['springer'].get('enabled', False):
            continue

        try:
            print(f"æ­£åœ¨ä» Springer è·å–è®ºæ–‡: {url}")
            feed = feedparser.parse(url)

            for entry in feed.entries[:max_results]:
                # æå–è®ºæ–‡ä¿¡æ¯
                authors = []
                if hasattr(entry, 'authors'):
                    authors = [author.get('name', '') for author in entry.authors]
                elif hasattr(entry, 'author'):
                    authors = [entry.author]

                papers_data.append({
                    "title": entry.get('title', '').strip(),
                    "authors": authors,
                    "abstract": entry.get('summary', '') or entry.get('description', ''),
                    "url": entry.get('link', ''),
                    "published": entry.get('published', ''),
                    "source": "Springer"
                })

            print(f"âœ“ ä» Springer è·å–åˆ° {len(feed.entries[:max_results])} ç¯‡è®ºæ–‡")

        except Exception as e:
            print(f"âš  Springer è·å–å¤±è´¥: {e}")

    return papers_data


def get_papers_from_semantic_scholar(topic: str, max_results=3) -> List[Dict]:
    """
    ä» Semantic Scholar API è·å–è®ºæ–‡ï¼ˆéœ€è¦ API Keyï¼‰
    """
    if not SOURCES_CONFIG['semantic_scholar'].get('enabled', False):
        return []

    api_key = SOURCES_CONFIG['semantic_scholar'].get('api_key')
    if not api_key:
        print("âš  æœªé…ç½® Semantic Scholar API Keyï¼Œè·³è¿‡")
        return []

    papers_data = []
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"

    try:
        print(f"æ­£åœ¨ä» Semantic Scholar æ£€ç´¢: {topic}")

        params = {
            'query': topic,
            'limit': max_results,
            'fields': 'title,authors,abstract,url,publicationDate',
            'year': datetime.datetime.now().year  # åªè·å–ä»Šå¹´çš„è®ºæ–‡
        }

        headers = {'x-api-key': api_key}
        response = httpx.get(base_url, params=params, headers=headers, timeout=30.0)

        if response.status_code == 200:
            data = response.json()
            for paper in data.get('data', []):
                authors = [author.get('name', '') for author in paper.get('authors', [])]

                papers_data.append({
                    "title": paper.get('title', ''),
                    "authors": authors,
                    "abstract": paper.get('abstract', '') or 'æ— æ‘˜è¦',
                    "url": paper.get('url', ''),
                    "published": paper.get('publicationDate', ''),
                    "source": "Semantic Scholar"
                })

            print(f"âœ“ ä» Semantic Scholar è·å–åˆ° {len(papers_data)} ç¯‡è®ºæ–‡")
        else:
            print(f"âš  Semantic Scholar API é”™è¯¯: {response.status_code}")

    except Exception as e:
        print(f"âš  Semantic Scholar è¯·æ±‚å¤±è´¥: {e}")

    return papers_data


def get_latest_papers_with_retry(topic="Large Language Models", max_results=3, max_retries=3):
    """
    ä» ArXiv è·å–æŒ‡å®šä¸»é¢˜çš„æœ€æ–°è®ºæ–‡ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    print(f"æ­£åœ¨æ£€ç´¢å…³äº {topic} çš„æœ€æ–°è®ºæ–‡...")

    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨æ–°çš„ Client APIï¼Œé…ç½®é¡µé¢å¤§å°å’Œå»¶è¿Ÿ
            client_arxiv = arxiv.Client(page_size=10, delay_seconds=10)
            search = arxiv.Search(
                query=topic,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )

            papers_data = []
            for result in client_arxiv.results(search):
                papers_data.append({
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "abstract": result.summary,
                    "url": result.entry_id,
                    "published": result.published
                })

            print(f"âœ“ æˆåŠŸè·å– {len(papers_data)} ç¯‡è®ºæ–‡")
            return papers_data

        except Exception as e:
            print(f"âš  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 10
                print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"âœ— æ— æ³•è·å–å…³äº '{topic}' çš„è®ºæ–‡")
                return []

def extract_clean_summary(content):
    """
    ä» API å“åº”ä¸­æå–å¹²å‡€çš„æ ¼å¼åŒ–æŠ¥å‘Š
    """
    lines = content.split('\n')

    # å¯»æ‰¾æŠ¥å‘Šå¼€å§‹æ ‡è®°ï¼ˆ## ğŸ“„ï¼‰
    start_idx = -1
    for i, line in enumerate(lines):
        if '## ğŸ“„' in line or 'è®ºæ–‡æ ‡é¢˜' in line:
            start_idx = i
            break

    if start_idx == -1:
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å¯»æ‰¾ç¬¬ä¸€ä¸ª ## æ ‡é¢˜
        for i, line in enumerate(lines):
            if line.strip().startswith('##'):
                start_idx = i
                break

    if start_idx == -1:
        # è¿˜æ²¡æ‰¾åˆ°ï¼Œå°±è¿”å›æ•´ä¸ªå†…å®¹
        return content

    # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä»å¼€å§‹ä½ç½®æå–æ‰€æœ‰å†…å®¹
    # åªåœ¨é‡åˆ°æ˜æ˜¾çš„æ€è€ƒè¿‡ç¨‹ç« èŠ‚æ ‡é¢˜æ—¶æ‰è¿›è¡Œè¿‡æ»¤
    result_lines = []
    skip_next_lines = False
    skip_reasoning_section = False

    for i in range(start_idx, len(lines)):
        line = lines[i]
        stripped = line.strip()

        # æ£€æµ‹æ€è€ƒè¿‡ç¨‹çš„ç« èŠ‚æ ‡é¢˜
        if (stripped.startswith('### 1.') or
            stripped.startswith('### 2.') or
            stripped.startswith('### 3.') or
            stripped.startswith('### 4.') or
            stripped.startswith('### 5.') or
            stripped.startswith('### 6.') or
            stripped in ['### åˆ†æ', '### æ­¥éª¤', '### æ€è·¯', '### è¾“å‡º',
                         '### åˆ†æè¯·æ±‚', '### æ€è€ƒè¿‡ç¨‹', '### æœ€ç»ˆè¾“å‡º',
                         '### **åˆ†æ**', '### **æ­¥éª¤**', '### **æ€è·¯**',
                         '### æœ€ç»ˆè¾“å‡ºç”Ÿæˆ']):
            skip_reasoning_section = True
            continue

        # å¦‚æœåœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œè·³è¿‡å¸¦ç¼–å·çš„åˆ—è¡¨é¡¹
        if skip_reasoning_section:
            if (stripped.startswith('1. **') or
                stripped.startswith('2. **') or
                stripped.startswith('3. **') or
                stripped.startswith('4. **') or
                stripped.startswith('5. **') or
                stripped.startswith('6. **')):
                continue

            # å¦‚æœé‡åˆ°æŠ¥å‘Šçš„ä¸»è¦ç« èŠ‚ï¼Œè¯´æ˜æ€è€ƒè¿‡ç¨‹ç»“æŸ
            if stripped.startswith('## ') or stripped.startswith('### ğŸ¯') or stripped.startswith('### ğŸ’¡') or stripped.startswith('### ğŸ§'):
                skip_reasoning_section = False

        # å¦‚æœä¸åœ¨è·³è¿‡æ¨¡å¼ï¼Œä¿ç•™è¿™ä¸€è¡Œ
        if not skip_reasoning_section:
            result_lines.append(line)

    return '\n'.join(result_lines).strip()


def validate_summary(content):
    """
    éªŒè¯æŠ¥å‘Šæ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„ç« èŠ‚
    """
    required_sections = [
        '### ğŸ¯ æ ¸å¿ƒæ‘˜è¦',
        '### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®',
        '### ğŸ§ ç®€è¯„ä¸å¯ç¤º'
    ]

    missing = []
    for section in required_sections:
        if section not in content:
            missing.append(section)

    return missing


def generate_summary(paper, max_retries=3):
    """
    è°ƒç”¨ NVIDIA API ç”Ÿæˆä¸­æ–‡è§£è¯»ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    print(f"æ­£åœ¨ç ”è¯»è®ºæ–‡ï¼š{paper['title']} ...")

    # ä½¿ç”¨æ›´ç®€æ´çš„ promptï¼Œæ˜ç¡®è¦æ±‚åªè¾“å‡ºæœ€ç»ˆç»“æœ
    prompt = f"""ä»»åŠ¡ï¼šç¿»è¯‘ä»¥ä¸‹è‹±æ–‡è®ºæ–‡æ‘˜è¦ä¸ºä¸­æ–‡æŠ€æœ¯ç®€æŠ¥ã€‚

å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š

## ğŸ“„ è®ºæ–‡æ ‡é¢˜ï¼š[ä¸­æ–‡ç¿»è¯‘çš„æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{paper['title']}
**ç¬¬ä¸€ä½œè€…**ï¼š{paper['authors'][0]} | **æœºæ„**ï¼šæœªçŸ¥

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[ç”¨ä¸­æ–‡ç¿»è¯‘è®ºæ–‡æ‘˜è¦ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§ï¼Œå†…å®¹è¦å®Œæ•´è¯¦ç»†]

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* [æ ¹æ®æ‘˜è¦åˆ—å‡º3ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹]
* [åˆ›æ–°ç‚¹2]
* [åˆ›æ–°ç‚¹3]

### ğŸ§ ç®€è¯„ä¸å¯ç¤º
[ä¸€å¥è¯æ€»ç»“è®ºæ–‡ä»·å€¼]

è®ºæ–‡æ‘˜è¦ï¼š{paper['abstract']}

è¾“å‡ºè¦æ±‚ï¼š
1. åªè¾“å‡ºæ ¼å¼åŒ–çš„æŠ¥å‘Šå†…å®¹
2. ä¸è¦è¾“å‡ºä»»ä½•åˆ†æã€æ€è€ƒè¿‡ç¨‹ã€æ­¥éª¤è¯´æ˜
3. ä¸è¦è¾“å‡º"å¥½çš„ï¼Œæˆ‘æ¥ç¿»è¯‘"ä¹‹ç±»çš„å¼€å¤´
4. ç›´æ¥ä»"## ğŸ“„"å¼€å§‹è¾“å‡º
5. ç¡®ä¿æ ¸å¿ƒæ‘˜è¦éƒ¨åˆ†æœ‰å®Œæ•´çš„å†…å®¹ï¼Œä¸è¦ç•™ç©º"""

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="z-ai/glm4.7",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIè®ºæ–‡ç¿»è¯‘åŠ©æ‰‹ã€‚åªè¾“å‡ºæ ¼å¼åŒ–çš„ä¸­æ–‡æŠ¥å‘Šï¼Œä¸è¦è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–åˆ†æè¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=3500,  # è¿›ä¸€æ­¥å¢åŠ åˆ° 3500
            )

            content = response.choices[0].message.content

            if content:
                # æ¸…ç†è¾“å‡ºï¼Œæå–å¹²å‡€çš„æŠ¥å‘Š
                cleaned = extract_clean_summary(content)

                # éªŒè¯æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€ç« èŠ‚
                missing = validate_summary(cleaned)
                if missing:
                    print(f"âš  æŠ¥å‘Šä¸å®Œæ•´ï¼Œç¼ºå°‘ç« èŠ‚: {', '.join(missing)}")
                    if attempt < max_retries - 1:
                        print(f"å°è¯• {attempt + 1} ä¸å®Œæ•´ï¼Œå‡†å¤‡é‡è¯•...")
                        time.sleep(30)
                        continue
                    else:
                        print(f"âš  å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›ä¸å®Œæ•´çš„ç»“æœ")

                return cleaned
            else:
                print("âš  API è¿”å›ç©ºå†…å®¹")
                return "âš  æ¨¡å‹è¿”å›ç©ºå†…å®¹ï¼Œè¯·é‡è¯•"

        except Exception as e:
            error_msg = str(e)
            print(f"âš  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                wait_time = 30
                print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                return f"âš  API è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®"

def send_to_wechat(message):
    """
    å°†æ¶ˆæ¯å‘é€åˆ°ä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äºº
    """
    if not WECHAT_WEBHOOK:
        print("âš  æœªé…ç½®ä¼ä¸šå¾®ä¿¡ Webhookï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")
        return False

    try:
        # ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯æ ¼å¼
        data = {
            "msgtype": "markdown",
            "markdown": {
                "content": message
            }
        }

        response = httpx.post(
            WECHAT_WEBHOOK,
            json=data,
            timeout=30.0
        )

        if response.status_code == 200:
            result = response.json()
            if result.get('errcode') == 0:
                print("âœ… å¾®ä¿¡æ¨é€æˆåŠŸ")
                return True
            else:
                print(f"âš  å¾®ä¿¡æ¨é€å¤±è´¥: {result.get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
                return False
        else:
            print(f"âš  å¾®ä¿¡æ¨é€è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
            return False

    except Exception as e:
        print(f"âš  å¾®ä¿¡æ¨é€å¼‚å¸¸: {e}")
        return False


def main():
    # 1. ä»å¤šä¸ªæ•°æ®æºè·å–è®ºæ–‡
    print("="*50)
    print("å¼€å§‹è·å–è®ºæ–‡æ•°æ®...")
    print("="*50)

    all_papers = []

    # 1.1 ä» ArXiv è·å–è®ºæ–‡
    if SOURCES_CONFIG['arxiv'].get('enabled', False):
        print("\nğŸ“š æ•°æ®æº: ArXiv")
        search_topics = SOURCES_CONFIG['arxiv'].get('search_topics', [])
        for i, topic in enumerate(search_topics):
            print(f"\n[{i+1}/{len(search_topics)}] å¤„ç†ä¸»é¢˜: {topic}")

            # æ¯æ¬¡ä¸»é¢˜ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
            if i > 0:
                print(f"ç­‰å¾… 15 ç§’åç»§ç»­ä¸‹ä¸€ä¸ªä¸»é¢˜...")
                time.sleep(15)

            papers = get_latest_papers_with_retry(topic=topic, max_results=1, max_retries=3)
            all_papers.extend(papers)

    # 1.2 ä» Springer è·å–è®ºæ–‡
    if SOURCES_CONFIG['springer'].get('enabled', False):
        print("\nğŸ“š æ•°æ®æº: Springer")
        springer_papers = get_papers_from_springer(max_results=2)
        all_papers.extend(springer_papers)

    # 1.3 ä» Semantic Scholar è·å–è®ºæ–‡
    if SOURCES_CONFIG['semantic_scholar'].get('enabled', False):
        print("\nğŸ“š æ•°æ®æº: Semantic Scholar")
        ss_topics = SOURCES_CONFIG['semantic_scholar'].get('search_topics', [])
        for topic in ss_topics:
            papers = get_papers_from_semantic_scholar(topic, max_results=1)
            all_papers.extend(papers)

    # å»é‡ï¼ˆåŸºäºURLï¼‰
    seen_urls = set()
    unique_papers = []
    for paper in all_papers:
        url = paper.get('url', '')
        if url not in seen_urls:
            seen_urls.add(url)
            unique_papers.append(paper)

    # é™åˆ¶æœ€å¤š5ç¯‡
    papers = unique_papers[:5]

    if not papers:
        print("\nâš  æœªèƒ½è·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œè¯·ç¨åé‡è¯•")
        return

    print(f"\nâœ“ å…±è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡\n")

    # ç»Ÿè®¡å„æ•°æ®æº
    source_stats = {}
    for paper in papers:
        source = paper.get('source', 'Unknown')
        source_stats[source] = source_stats.get(source, 0) + 1

    daily_report = f"# ğŸ“… AI å‰æ²¿è®ºæ–‡æ—¥æŠ¥ ({datetime.date.today()})\n\n"
    daily_report += f"**ä¸»é¢˜**: å¤§è¯­è¨€æ¨¡å‹ã€æ™ºèƒ½ä½“ã€å¢å¼ºå‹LLMæ¨ç†å’Œæ¨ç†ä¼˜åŒ–\n\n"
    daily_report += f"**æ•°æ®æº**: {', '.join(source_stats.keys())}\n\n"
    daily_report += f"ä»Šæ—¥ä¸ºæ‚¨ç²¾é€‰ {len(papers)} ç¯‡æœ€æ–°è®ºæ–‡\n\n"

    # 2. é€ç¯‡å¤„ç†
    for i, paper in enumerate(papers, 1):
        print(f"\nå¤„ç†ç¬¬ {i}/{len(papers)} ç¯‡è®ºæ–‡...")
        summary = generate_summary(paper, max_retries=3)

        # æ‹¼æ¥å†…å®¹
        daily_report += f"{summary}\n"
        daily_report += f"ğŸ”— **åŸæ–‡é“¾æ¥**: {paper['url']}\n"
        if 'source' in paper:
            daily_report += f"ğŸ“š **æ¥æº**: {paper['source']}\n"
        daily_report += "---\n\n"

    # 3. è¾“å‡ºç»“æœ
    print("\n" + "="*20 + " ç”Ÿæˆç»“æœ " + "="*20 + "\n")
    print(daily_report)

    # 4. å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    with open('daily_report.md', 'w', encoding='utf-8') as f:
        f.write(daily_report)
    print("\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° daily_report.md")

    # 5. æ¨é€åˆ°å¾®ä¿¡ï¼ˆå¦‚æœé…ç½®äº† Webhookï¼‰
    if WECHAT_WEBHOOK:
        print("\næ­£åœ¨æ¨é€åˆ°å¾®ä¿¡...")
        # ç”Ÿæˆé€‚åˆå¾®ä¿¡çš„æ¶ˆæ¯æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        wechat_message = f"## ğŸ“… AI å‰æ²¿è®ºæ–‡æ—¥æŠ¥ ({datetime.date.today()})\n\n"
        wechat_message += f"**ä¸»é¢˜**: å¤§è¯­è¨€æ¨¡å‹ã€æ™ºèƒ½ä½“ã€å¢å¼ºå‹LLMæ¨ç†å’Œæ¨ç†ä¼˜åŒ–\n\n"
        wechat_message += f"**æ•°æ®æº**: {', '.join(source_stats.keys())}\n\n"
        wechat_message += f"ä»Šæ—¥ä¸ºæ‚¨ç²¾é€‰ {len(papers)} ç¯‡æœ€æ–°è®ºæ–‡\n\n"

        # ä»å·²ç”Ÿæˆçš„æŠ¥å‘Šä¸­æå–è®ºæ–‡æ ‡é¢˜
        import re
        title_pattern = r'## ğŸ“„ è®ºæ–‡æ ‡é¢˜ï¼š(.*?)\n'
        titles = re.findall(title_pattern, daily_report)

        for i, title in enumerate(titles, 1):
            wechat_message += f"**{i}. {title}**\n\n"

        # æ·»åŠ  GitHub é“¾æ¥ï¼ˆéœ€è¦ç”¨æˆ·æ›¿æ¢ä¸ºè‡ªå·±çš„ä»“åº“åœ°å€ï¼‰
        wechat_message += f"\nğŸ“® [ç‚¹å‡»æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š](https://github.com/A-pricity/Daily_AI_Paper_Bot/blob/main/daily_report.md)"

        send_to_wechat(wechat_message)
    else:
        print("\nâš  æœªé…ç½®ä¼ä¸šå¾®ä¿¡ Webhookï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")

    # å…³é—­ http å®¢æˆ·ç«¯
    http_client.close()

if __name__ == "__main__":
    main()
