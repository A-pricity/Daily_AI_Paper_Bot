# 📅 AI 前沿论文日报 (2026-02-08)

**主题**: 大语言模型、智能体、增强型LLM推理和推理优化

今日为您精选 3 篇最新论文

## 📄 论文标题：共享LoRA子空间用于近乎严格的持续学习
**原标题**：Shared LoRA Subspaces for almost Strict Continual Learning
**第一作者**：Prakhar Kaushik | **机构**：未知

### 🎯 核心摘要
高效且持续地将大型预训练模型适应新任务对于实际部署至关重要，但由于灾难性遗忘和再训练的高昂成本，这仍然是一个挑战。虽然像低秩适应（LoRA）这样的参数高效微调方法降低了计算需求，但它们缺乏在不依赖数据回放或多适配器的情况下进行严格持续学习和知识整合的机制。我们提出了Share，这是一种参数高效的持续微调新方法，它学习并动态更新一个单一的、共享的低秩子空间，从而实现跨多个任务和模态的无缝适应。Share构建了一个基础子空间，该子空间从过去的任务中提取核心知识，并通过识别必要的子空间方向来增量整合新信息。每个新任务的知识被整合到这个不断演变的子空间中，促进了前向知识转移，同时最大限度地减少了灾难性干扰。该方法相比传统LoRA方法实现了高达100倍的参数减少和281倍的内存节省，同时保持了与联合训练模型相当的性能。单个Share模型可以取代数百个特定任务的LoRA适配器，支持可扩展的、异步的持续学习。在图像分类、自然语言理解、3D姿态估计和文本到图像生成方面的实验验证了其有效性，使Share成为大规模AI系统中终身学习的实用且可扩展的解决方案。

### 💡 核心创新点与贡献
* 提出了“Share”框架，通过学习并动态更新一个单一的共享低秩子空间，实现了无需数据回放或多适配器的参数高效持续微调。
* 构建了能够提取过去任务核心知识的基础子空间，并通过识别关键子空间方向增量整合新信息，有效促进了前向知识转移并最小化灾难性干扰。
* 在保持与联合训练模型相当性能的前提下，实现了相比传统LoRA方法高达100倍的参数减少和281倍的内存节省，支持跨多模态的可扩展异步持续学习。

### 🧐 简评与启示
Share通过共享低秩子空间机制成功解决了持续学习中的灾难性遗忘与参数效率难题，为大规模AI系统的终身学习提供了一种极具实用性和扩展性的新范式。
🔗 **原文链接**: http://arxiv.org/abs/2602.06043v1
---

## 📄 论文标题：SwimBird：在混合自回归MLLM中激发可切换推理模式
**原标题**：SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs
**第一作者**：Jintao Tong | **机构**：未知

### 🎯 核心摘要
多模态大语言模型通过连接视觉和语言，在多模态感知和推理方面取得了显著进展。然而，大多数现有的MLLM主要使用文本思维链进行推理，这限制了它们在视觉密集型任务上的有效性。最近的方法将固定数量的连续隐藏状态作为“视觉思维”注入推理过程，并提高了视觉性能，但往往以牺牲基于文本的逻辑推理为代价。我们认为，核心局限性在于僵化的、预定义的推理模式，无法针对不同的用户查询自适应地选择最合适的思维模态。我们介绍了SwimBird，一种可切换推理模式的MLLM，它根据输入动态地在三种推理模式之间切换：(1) 纯文本推理，(2) 纯视觉推理（将连续隐藏状态作为视觉思维），以及 (3) 交错视觉-文本推理。为了实现这一能力，我们采用了一种混合自回归公式，统一了文本思维的下一个token预测和视觉思维的下一个嵌入预测，并设计了一个系统的推理模式策展策略来构建SwimBird-SFT-92K，这是一个涵盖所有三种推理模式的多样化监督微调数据集。通过实现灵活的、查询自适应的模式选择，SwimBird在显著提高视觉密集型任务性能的同时，保持了强大的文本逻辑能力。在涵盖文本推理和具有挑战性的视觉理解的多样化基准测试中的实验表明，SwimBird取得了最先进的结果，并且比以前固定模式的多模态推理方法有稳健的增益。

### 💡 核心创新点与贡献
* 提出了SwimBird，一种能够根据输入动态在纯文本、纯视觉和交错视觉-文本三种推理模式间切换的MLLM，打破了固定推理模式的局限性。
* 采用混合自回归公式，统一了针对文本思维的下一个token预测和针对视觉思维的下一个嵌入预测，实现了不同模态思维的统一建模。
* 设计了系统的推理模式策展策略，构建了包含92K样本的SwimBird-SFT-92K数据集，全面覆盖了三种推理模式以支持模型训练。

### 🧐 简评与启示
SwimBird通过自适应的模式切换机制有效解决了多模态模型中视觉感知与文本逻辑难以兼得的难题，实现了推理灵活性与性能的双重提升。
🔗 **原文链接**: http://arxiv.org/abs/2602.06040v1
---

## 📄 论文标题：基于视角描述预测相机位姿以实现空间推理
**原标题**：Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning
**第一作者**：Xuejun Zhang | **机构**：未知

### 🎯 核心摘要
对于当前的多模态大语言模型而言，多图像空间推理仍然是一项挑战。虽然单视图感知本质上是二维的，但对多个视图进行推理需要在视点之间建立连贯的场景理解。特别是，我们研究了视角采样任务，该任务要求模型必须从多视点观察中建立连贯的三维理解，并利用这种理解从新的、语言指定的视点进行推理。我们介绍了CAMCUE，这是一个位姿感知的多图像框架，它使用相机位姿作为跨视图融合和新视图推理的显式几何锚点。CAMCUE将每个视图的位姿注入到视觉标记中，将自然语言视角描述映射到目标相机位姿，并合成一个位姿条件的想象目标视图来支持回答。为了支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例将多视图图像和位姿与多样化的目标视点描述及视角转移问题配对。我们还在测试集中包含了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确性提高了9.06%，并且能够根据自然语言视角描述预测目标位姿，在20°误差范围内的旋转准确率和0.5误差阈值内的平移准确率均超过90%。这种直接映射避免了昂贵的测试时搜索和匹配过程，将每个示例的推理时间从256.6秒减少到1.45秒，从而在现实场景中实现快速、交互式的应用。

### 💡 核心创新点与贡献
* 提出了CAMCUE框架，利用相机位姿作为显式几何锚点，有效解决了多模态模型在跨视图融合和新视角推理方面的难题。
* 设计了将位姿信息注入视觉标记、将自然语言视角描述映射到目标相机位姿并合成位姿条件想象视图的机制。
* 构建了包含丰富视角描述和视角转移问题的CAMCUE-DATA数据集，并通过直接映射策略避免了昂贵的测试时搜索匹配，将推理效率提升了两个数量级。

### 🧐 简评与启示
该研究通过引入显式几何位姿信息，有效突破了多模态大语言模型在多视图空间推理中的瓶颈，在大幅提升精度的同时实现了推理速度的质的飞跃。
🔗 **原文链接**: http://arxiv.org/abs/2602.06041v1
---

